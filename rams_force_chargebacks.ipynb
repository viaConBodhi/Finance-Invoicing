{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773088a-96c7-4d9b-9df9-e9084f1d6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime as dt\n",
    "import smartsheet\n",
    "import os\n",
    "import re\n",
    "from sql_utliz import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd35a10-1a5b-47c4-9dc2-40181a9945c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'smartsheet_key'\n",
    "monthly_billing = 'smartsheet_id'\n",
    "source_billable_records_sheet = 'smartsheet_id'\n",
    "variance_report = 'smartsheet_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c140a-2037-46eb-acfc-543c993d9a3c",
   "metadata": {},
   "source": [
    "# Load the Jira tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c69ccb-51d3-46e3-a77b-97fc282cd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection settings\n",
    "server = 'server_path'\n",
    "database = 'database_name'\n",
    "username = 'username'\n",
    "password = 'password'\n",
    "BIC_Issues = 'dbo.table_name'\n",
    "Top_Level_Issues = 'tablename' # load the BIC_Top_Level_Issues table to extact the required data\n",
    "BIC_User_Time ='tablename'\n",
    "\n",
    "# Establishing the database connection\n",
    "conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# SQL query to fetch the view data\n",
    "query1 = f'SELECT * FROM {BIC_Issues}'\n",
    "query2 = f'SELECT * FROM {Top_Level_Issues}'\n",
    "query3 = f'SELECT * FROM {BIC_User_Time}'\n",
    "\n",
    "# Executing the query and fetching the data\n",
    "bic_issues_df = pd.read_sql_query(query1, conn)\n",
    "top_level_issues_df = pd.read_sql_query(query2, conn)\n",
    "bic_user_time_df = pd.read_sql_query(query3, conn)\n",
    "\n",
    "# Closing the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b9848-6226-40a6-9d60-dd77f3094f9f",
   "metadata": {},
   "source": [
    "# Identify top level records that have an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54988d9-e13a-4af9-8896-01cbe7bfe41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_project_key_types = ['UTT', 'RDS']\n",
    "target_project_key_records = top_level_issues_df[top_level_issues_df.Project_Key.isin(target_project_key_types)]\n",
    "top_level_records_with_index = target_project_key_records[target_project_key_records.Research_Info__Account_Num.notna()]\n",
    "top_level_records_with_index_id_list = list(set(list(top_level_records_with_index.Issue_ID)))\n",
    "len(top_level_records_with_index_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea10ad5-782c-4534-9e04-1c10a321f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# slice out of the top level table the fields needed\n",
    "top_level_required_fields = top_level_records_with_index[['Issue_ID', \n",
    "                                                          'Issue_Ticket_Number',\n",
    "                                                          'Project_Name',\n",
    "                                                          'Project_Key',\n",
    "                                                          'Issue_Status',\n",
    "                                                          'Research_Info__Project_Title',\n",
    "                                                          'Research_Info__PI_Name',\n",
    "                                                          'Research_Info__Project_Description',\n",
    "                                                          'Research_Info__IRB_Num',\n",
    "                                                          'Research_Info__Account_Num']]\n",
    "\n",
    "\n",
    "# slice out only the required fields from the bic_issues table and filter down to the target records\n",
    "bic_issues_required_fields = bic_issues_df[['Issue_ID',\n",
    "                                            'Parent_Issue_ID',\n",
    "                                            'Issue_Ticket_Number',\n",
    "                                            #'Billable_Time_Seconds',\n",
    "                                            'Project_Key',\n",
    "                                            'Issue_Summary',\n",
    "                                            'Issue_Description',\n",
    "                                            'Issue_Status']]\n",
    "                                            #'Created_Date',\n",
    "                                            #'Updated_Date',\n",
    "                                            #'Due_Date',\n",
    "                                            #'Resolution_Date']]\n",
    "\n",
    "\n",
    "# update the parent table field titles so they can be merged with without causing title problems\n",
    "top_level_required_fields.rename(columns={'Issue_Ticket_Number':'Issue_Ticket_Number_parent', \n",
    "                                          'Issue_Status':'Issue_Status_parent',\n",
    "                                          'Project_Key':'Project_Key_parent',                                           \n",
    "                                          'Issue_Summary':'Issue_Summary_parent',\n",
    "                                          'Issue_Description':'Issue_Description_parent',\n",
    "                                          'Issue_Status':'Issue_Status_parent',\n",
    "                                          'Created_Date':'Created_Date_parent',\n",
    "                                          'Updated_Date':'Updated_Date_parent',\n",
    "                                          'Due_Date':'Due_Date_parent',\n",
    "                                          'Resolution_Date':'Resolution_Date_parent'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d5dda-fced-4eff-a18f-5498f1155fd6",
   "metadata": {},
   "source": [
    "# Remove subtask records that are not billed to the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7de08-b288-4a63-9166-d7d4bf12495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_title_list = ['SLA (Sub of RDS','SLA','Exploratory (Sub of RDS','Quality Control (Sub of RDS','Compliance Review (Sub of RDS']\n",
    "\n",
    "def records_to_remove(text):\n",
    "    for keyword in remove_title_list:\n",
    "        if keyword in text:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# add a column that identifies if the 'Issue_Summary' field contains the text SLA and returns a 1 if SLA is in the text\n",
    "bic_issues_required_fields['remove_record'] = bic_issues_required_fields.apply(lambda x: records_to_remove(x.Issue_Summary), axis=1)\n",
    "\n",
    "# filter to remove records that should not have time aggrogated for billing \n",
    "bic_issues_id_remove_df = bic_issues_required_fields[bic_issues_required_fields.remove_record == 1]\n",
    "bic_issues_id_remove_list = list(set(list(bic_issues_id_remove_df.Issue_ID))) \n",
    "bic_issues_required_fields = bic_issues_required_fields[bic_issues_required_fields.remove_record == 0]\n",
    "\n",
    "#bic_issues_required_fields.drop(columns=['remove_record'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba15266a-f249-4e1c-8c4a-8aa9c87488b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# replace the \"NaN\" values in the Parent_Issue_ID field with the values from the Issue_ID field to help with identifiing and merging\n",
    "# since it appears the missing values are due to these records being the parent records\n",
    "bic_issues_required_fields['Parent_Issue_ID'].fillna(bic_issues_required_fields['Issue_ID'], inplace=True)\n",
    "\n",
    "bic_issues_target_records = bic_issues_required_fields[bic_issues_required_fields.Parent_Issue_ID.isin(top_level_records_with_index_id_list)]\n",
    "len(bic_issues_target_records)\n",
    "#bic_issues_target_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf608c38-204c-4f84-8b08-39676d5251a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e824b78-f726-414a-b73a-972f2b7dec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice out the required fields from the bic_user_time df\n",
    "bic_user_time_required_fields = bic_user_time_df[['WorkLog_ID', \n",
    "                                                  'Top_Level_Issue_ID', \n",
    "                                                  'Work_Logged_Issue_ID', \n",
    "                                                  'User_Display_Name', \n",
    "                                                  'Work_Start',\n",
    "                                                  'Time_Worked_Seconds', \n",
    "                                                  'Time_Is_Billable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da4e6c-0adf-45ec-8ef0-8313184eae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the bic_user_time table has a column 'Top_Level_Issue_ID' that list the parent record for the logged work which appears never to be blank\n",
    "# and the \"Work_Logged_Issue_ID appears to log where the work was attributed to, which could be the parent ticket, \n",
    "# return bic user time records that have parent IDs that include a billable index and have time listed as 'billable'\n",
    "bic_user_time_with_billable_index = bic_user_time_required_fields[(bic_user_time_required_fields.Top_Level_Issue_ID.isin(top_level_records_with_index_id_list)) & (bic_user_time_required_fields.Time_Is_Billable==1)]\n",
    "\n",
    "# we then need to remove work time attributed to subtasks that are not billable using the listing identified prior\n",
    "bic_user_time_with_billable_index = bic_user_time_with_billable_index[~bic_user_time_with_billable_index.Work_Logged_Issue_ID.isin(bic_issues_id_remove_list)]\n",
    "len(bic_user_time_with_billable_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261338aa-1e54-4506-bb98-0b2d429f0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that derivers the billable time in hours and fractions of hours\n",
    "bic_user_time_with_billable_index['Time_Worked_Hours'] = (bic_user_time_with_billable_index.Time_Worked_Seconds/60)/60\n",
    "\n",
    "# convert the 'Work_Start' field to datetime to enable sorting by date\n",
    "bic_user_time_with_billable_index['Work_Start'] = pd.to_datetime(bic_user_time_with_billable_index['Work_Start'])\n",
    "\n",
    "bic_user_time_with_billable_index.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3319d3c-ca39-4932-ba76-fb088241fc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a9cfc84-be20-4de7-b8b4-7a83b9588fc1",
   "metadata": {},
   "source": [
    "# Add a field that applies the 8 hours gratis based on the first 8 billable hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5a647-8e28-46b3-bba4-aa2e41eae2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a unique listing of the Top_Level_Issue_ID so this can be used as a filter to return only records for the parent ticket\n",
    "parent_ticket_unique_list = list(set(list(bic_user_time_with_billable_index.Top_Level_Issue_ID)))\n",
    "\n",
    "\n",
    "# helper funtion for accounting for the gratis time\n",
    "def distribute_gratis_hours(dataframe, gratis):\n",
    "    # Check if the DataFrame is empty or gratis is non-positive\n",
    "    if dataframe.empty or gratis <= 0:\n",
    "        return dataframe\n",
    "\n",
    "    # Initialize a new column \"gratis_hour\" with zeros\n",
    "    dataframe[\"gratis_time\"] = 0\n",
    "\n",
    "    # Iterate through rows and distribute gratis hours\n",
    "    for index, row in dataframe.iterrows():\n",
    "        time_worked_hours = row[\"Time_Worked_Hours\"]\n",
    "        remaining_gratis = gratis - row[\"gratis_time\"]\n",
    "\n",
    "        # If there are gratis hours remaining and enough \"Time_Worked_Hours\" in this row\n",
    "        if remaining_gratis > 0 and time_worked_hours > 0:\n",
    "            # Calculate the amount of gratis hours to add to this row\n",
    "            if time_worked_hours >= remaining_gratis:\n",
    "                dataframe.at[index, \"gratis_time\"] += remaining_gratis\n",
    "            else:\n",
    "                dataframe.at[index, \"gratis_time\"] += time_worked_hours\n",
    "\n",
    "            # Update the remaining gratis hours\n",
    "            gratis -= dataframe.at[index, \"gratis_time\"]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# loop over the billable records that are filtered per the parent ticket and apply the gratis time\n",
    "df_hold = []\n",
    "for parent_ticket in parent_ticket_unique_list:\n",
    "    \n",
    "    # return a listing of records related by the parent ticket \n",
    "    parent_record_set = bic_user_time_with_billable_index[bic_user_time_with_billable_index.Top_Level_Issue_ID == parent_ticket]\n",
    "    parent_record_set = parent_record_set.sort_values(by=['Work_Start'], ascending=True)\n",
    "    \n",
    "    # get the len of the parent record set so this can be used for the while loop\n",
    "    parent_record_set_len = (parent_record_set)\n",
    "    \n",
    "    parent_record_set = distribute_gratis_hours(parent_record_set, 8)        \n",
    "    \n",
    "    df_hold.append(parent_record_set)\n",
    "    \n",
    "\n",
    "# get an updated data frame that adds the column gratis_time that applies the required gratis time per record\n",
    "bic_user_time_with_billable_index = pd.concat(df_hold)\n",
    "bic_user_time_with_billable_index.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe14703-519d-432e-9f53-433aeea7aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper column that identifies if Top_Level_Issue_ID and Work_Logged_Issue_ID have the same value as this indicates the record \n",
    "# has time applied to the parent ticket. The records with matching values will be sliced from those without so the parent ticket fields can \n",
    "# be merged with the parent ticket data then the records with values that do not match can have the parent ticket data merged based on the \n",
    "# \"Top_Level_Issue_ID field and then have the subtask data merged on the \"Work_Logged_Issue_ID\" field. Finally, the two seperate DFs will be \n",
    "# concat resulting in records that are subtasks having both parent and subtask features while records with only parent ticket data will have \n",
    "# missing values for any subtask fields\n",
    "\n",
    "bic_user_time_with_billable_index['matching_issue_ids'] = bic_user_time_with_billable_index.apply(lambda x: 1 if x.Top_Level_Issue_ID == x.Work_Logged_Issue_ID else 0, axis=1)\n",
    "\n",
    "issue_ids_match_records = bic_user_time_with_billable_index[bic_user_time_with_billable_index.matching_issue_ids == 1]\n",
    "issue_ids_match_records.drop(columns=['matching_issue_ids'], inplace=True)\n",
    "\n",
    "issue_ids_no_match_records = bic_user_time_with_billable_index[bic_user_time_with_billable_index.matching_issue_ids == 0]\n",
    "issue_ids_no_match_records.drop(columns=['matching_issue_ids'], inplace=True)\n",
    "\n",
    "merge_issue_ids_match_records_parent_features = pd.merge(issue_ids_match_records,top_level_required_fields, left_on='Work_Logged_Issue_ID', right_on='Issue_ID', how='left')\n",
    "merge_issue_ids_match_records_sub_feature = pd.merge(issue_ids_no_match_records,bic_issues_required_fields, left_on='Work_Logged_Issue_ID', right_on='Issue_ID', how='left')\n",
    "merge_issue_ids_match_records_sub_feature = pd.merge(merge_issue_ids_match_records_sub_feature,top_level_required_fields, left_on='Top_Level_Issue_ID', right_on='Issue_ID', how='left')\n",
    "\n",
    "final_record_set = pd.concat([merge_issue_ids_match_records_parent_features,merge_issue_ids_match_records_sub_feature], sort=False)\n",
    "\n",
    "# Create a new column 'month_year_of_hours_worked' with year and month\n",
    "final_record_set['Work_Start'] = pd.to_datetime(final_record_set['Work_Start'])\n",
    "final_record_set['month_year_of_hours_worked'] = final_record_set['Work_Start'].dt.to_period('M')\n",
    "# create a filter to only use records from 2 months ago (current month +1) to account for changes with the billing\n",
    "target_processing_month_year = pd.to_datetime(dt.now()).to_period('M')-2\n",
    "\n",
    "# Filter records with the same year and month as the current year and month\n",
    "final_record_set = final_record_set[final_record_set['month_year_of_hours_worked'] <= target_processing_month_year]\n",
    "final_record_set = final_record_set.sort_values(by='Work_Start', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c91dd-5ad6-4656-a151-788a27498b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_dataframe_as_excel(df, index, month_effort_accrued):\n",
    "    # Check if the 'delete_me' directory exists, and create it if not\n",
    "    if not os.path.exists('delete_me'):\n",
    "        os.makedirs('delete_me')\n",
    "\n",
    "    # Define a regular expression pattern for a valid index\n",
    "    valid_index_pattern = r'^[A-Za-z]*\\d+$'\n",
    "\n",
    "    # Check if the index matches the valid pattern\n",
    "    if re.match(valid_index_pattern, index):\n",
    "        # Define the Excel file name based on the index and month_effort_accrued\n",
    "        excel_file_name = f'delete_me/{index}_{month_effort_accrued}.xlsx'\n",
    "    else:\n",
    "        # If the index doesn't match the pattern, set it to \"ERROR\"\n",
    "        index = \"ERROR\"\n",
    "        # Define a generic Excel file name\n",
    "        excel_file_name = f'delete_me/{index}_{month_effort_accrued}.xlsx'\n",
    "\n",
    "    # Save the DataFrame as an Excel file\n",
    "    df.to_excel(excel_file_name, index=False)\n",
    "\n",
    "    return excel_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347abc4-7caf-4667-baef-77d13b10acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_hours_worked_per_month(df):\n",
    "    # Convert 'Work_Start' column to datetime if it's not already\n",
    "    df['Work_Start'] = pd.to_datetime(df['Work_Start'])\n",
    "    \n",
    "    # create a new field called hours_less_gratis that will be used to calculate the billable hours\n",
    "    df['hours_less_gratis'] = df.Time_Worked_Hours - df.gratis_time\n",
    "    \n",
    "    # get a unique list of the values in Research_Info__Account_Num so they can be used as filter\n",
    "    index_list = list(set(list(df.Research_Info__Account_Num)))\n",
    "    \n",
    "    # list to hold the output dfs \n",
    "    processed_dfs_list = []\n",
    "    \n",
    "    # use the index as a filter to return only records for that index\n",
    "    for idx in index_list:\n",
    "        \n",
    "        index_df = df[df.Research_Info__Account_Num == idx]\n",
    "        \n",
    "        # get a list of the month_year_of_hours_worked dates to use as a filter to group the records\n",
    "        work_date_list = list(set(list(index_df.month_year_of_hours_worked)))\n",
    "        \n",
    "        # list to hold the processed dfs\n",
    "        loop_df_list = []\n",
    "        \n",
    "        \n",
    "        # filter based on each of the available dates in the work_date_list\n",
    "        for date in work_date_list:\n",
    "            # filter records to group based on the month/year \n",
    "            date_df = index_df[index_df.month_year_of_hours_worked == date]\n",
    "            \n",
    "            monthly_sum = date_df.hours_less_gratis.sum()\n",
    "            \n",
    "            \n",
    "            # Select the columns to slice/keep \n",
    "            columns_to_keep = ['Research_Info__Account_Num', 'Project_Key_parent', 'Issue_Ticket_Number_parent',\n",
    "                               'Issue_Status_parent', 'Research_Info__Project_Title', 'Research_Info__PI_Name',\n",
    "                               'Research_Info__Project_Description', 'Research_Info__IRB_Num', 'month_year_of_hours_worked']\n",
    "\n",
    "            loop_result = date_df[columns_to_keep].iloc[:1]\n",
    "            loop_result['hours_worked_in_month'] = monthly_sum\n",
    "            loop_df_list.append(loop_result)\n",
    "            \n",
    "            # save the result as a pdf for invoicing \n",
    "            #save_dataframe_as_excel(loop_result, idx, date)\n",
    "            save_dataframe_as_excel(date_df, idx, date)\n",
    "            \n",
    "        # concat the list\n",
    "        concat_list = pd.concat(loop_df_list)\n",
    "        processed_dfs_list.append(concat_list)\n",
    "        \n",
    "    results_df = pd.concat(processed_dfs_list)\n",
    "    return results_df\n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45535364-a638-4f28-806f-5b7d50fe0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Call the function to calculate hours worked per month\n",
    "result_df = calculate_hours_worked_per_month(final_record_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd7dda-14ac-44a8-aaa0-d5626a2b7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add the date/time the records were processed \n",
    "batched_time = dt.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "result_df['processed_on'] = batched_time\n",
    "final_record_set['processed_on'] = batched_time # records that were the original source records that were cleaned for processing\n",
    "result_df = result_df.sort_values('month_year_of_hours_worked', ascending = True)\n",
    "\n",
    "# add the field for RamsForce processing status\n",
    "result_df['ramsforce_status'] = 'Ready to Process'\n",
    "\n",
    "result_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc27bc8-3696-4bd8-a84a-cdc5f49c6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_name_to_id(sheet_id, api_key):\n",
    "    # Initialize the Smartsheet client\n",
    "    smartsheet_client = smartsheet.Smartsheet(api_key)\n",
    "\n",
    "    try:\n",
    "        # Load the specified sheet\n",
    "        sheet = smartsheet_client.Sheets.get_sheet(sheet_id)\n",
    "\n",
    "        # Initialize an empty dictionary to store column name to column ID mapping\n",
    "        column_name_to_id = {}\n",
    "\n",
    "        # Iterate through columns in the sheet\n",
    "        for column in sheet.columns:\n",
    "            # Store the column name (title) as the key and column ID as the value\n",
    "            column_name_to_id[column.title] = column.id\n",
    "\n",
    "        return column_name_to_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cf30f-cd9f-4968-b305-17a60b98981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_and_load_to_dataframe_with_row_id(api_token, sheet_id):\n",
    "    # Initialize the Smartsheet client\n",
    "    ss_client = smartsheet.Smartsheet(api_token)\n",
    "\n",
    "    try:\n",
    "        # Get the sheet information to access column details\n",
    "        sheet = ss_client.Sheets.get_sheet(sheet_id)\n",
    "        columns = sheet.columns\n",
    "\n",
    "        # Extract column headers from the sheet\n",
    "        column_headers = [column.title for column in columns]\n",
    "\n",
    "        # Load the entire sheet\n",
    "        sheet = ss_client.Sheets.get_sheet(sheet_id, page_size=10000)\n",
    "        rows = sheet.rows\n",
    "\n",
    "        # Create a list to hold row data\n",
    "        data = []\n",
    "\n",
    "        # Extract data from each row and add to the list\n",
    "        for row in rows:\n",
    "            row_data = []\n",
    "            \n",
    "            # Add the Smartsheet row ID as the first column\n",
    "            row_data.append(row.id)\n",
    "            \n",
    "            for cell in row.cells:\n",
    "                row_data.append(cell.display_value)\n",
    "            data.append(row_data)\n",
    "\n",
    "        # Add \"row_id\" as the column name for the Smartsheet row ID\n",
    "        column_headers.insert(0, \"row_id\")\n",
    "\n",
    "        # Convert the list to a DataFrame with column headers\n",
    "        df = pd.DataFrame(data, columns=column_headers)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5ed3d-725e-4b5c-8ddc-f76440b321cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_records_to_smartsheet(df, api_key, sheet_unique_id):\n",
    "    \n",
    "    # get the smartsheet column ids \n",
    "    col_ids = get_column_name_to_id(sheet_unique_id, api_key)\n",
    "    \n",
    "    # create a copy of the input df\n",
    "    scoped_records = df.copy()\n",
    "    \n",
    "    # Initialize the Smartsheet client\n",
    "    smartsheet_client = smartsheet.Smartsheet(api_key)\n",
    "\n",
    "    # Convert all columns to string data type\n",
    "    new_records = scoped_records.astype(str)\n",
    "\n",
    "    new_records_columns = list(set(list(scoped_records)))\n",
    "    df_len = len(new_records)\n",
    "    counter = 0\n",
    "    # use a counter to iterate over the rows within the dataframe to load the data from each row into the smartsheet format\n",
    "    while counter < df_len:\n",
    "        # Specify cell values for one row\n",
    "        row_a = smartsheet.models.Row()\n",
    "        row_a.to_top = True\n",
    "        # iterate over the field names in the scoped dataset and use these field names to call the \n",
    "        # key from the result dictionary that contains the smartsheet field ID associated with the key/title\n",
    "        # and use this field ID to build the record data neede for smartsheet to post the row\n",
    "        for column_name in new_records_columns:\n",
    "            row_a.cells.append({\n",
    "                'column_id': int(col_ids[column_name]), \n",
    "                'value': new_records.iloc[counter][column_name]\n",
    "            })\n",
    "        # Add rows to sheet\n",
    "        response = smartsheet_client.Sheets.add_rows(\n",
    "            sheet_id=sheet_unique_id,  \n",
    "            list_of_rows=[row_a],\n",
    "        )\n",
    "\n",
    "        # Check for success or handle errors as needed\n",
    "        if response.message == 'SUCCESS':\n",
    "            print('Rows added successfully')\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            print(f'Error: {response.message}')\n",
    "            counter = counter + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55a623-d9cc-4643-872c-ee0182caa5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_excel_file_to_row(api_token, sheet_id, row_id, excel_file_path):\n",
    "    try:\n",
    "        # Initialize the Smartsheet client\n",
    "        smartsheet_client = smartsheet.Smartsheet(api_token)\n",
    "\n",
    "        # Check if the Excel file exists at the specified path\n",
    "        if not os.path.isfile(excel_file_path):\n",
    "            print(f\"Excel file not found at path: {excel_file_path}\")\n",
    "            return False\n",
    "\n",
    "        # Extract the file name from the path\n",
    "        file_name = os.path.basename(excel_file_path)\n",
    "\n",
    "        # Attach the Excel file to the row using smartsheet_client.Attachments.attach_file_to_row\n",
    "        response = smartsheet_client.Attachments.attach_file_to_row(\n",
    "            sheet_id,\n",
    "            row_id,\n",
    "            (file_name, open(excel_file_path, 'rb'), 'application/ms-excel')\n",
    "        )\n",
    "\n",
    "        # Check if the file was successfully attached\n",
    "        if response.message == 'SUCCESS':\n",
    "            print(f\"Excel file attached to row {row_id} successfully.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error attaching Excel file to row {row_id}.\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dce614-7c13-492f-b833-f55beb4b3481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_dataframes(df1, df2, col_name_1, col_name_2):\n",
    "    # create a compound key based on the unique worklog id and the time worked to enable filtering to find if there has been an update to the time entry for the worklog \n",
    "    df1_copy = df1.copy()\n",
    "    df2_copy = df2.copy()\n",
    "    df1_copy['compound_loc'] = df1_copy.apply(lambda x: str(x[f'{col_name_1}'])+\"_\"+str(x[f'{col_name_2}']), axis=1)\n",
    "    df2_copy['compound_loc'] = df2_copy.apply(lambda x: str(x[f'{col_name_1}'])+\"_\"+str(x[f'{col_name_2}']), axis=1)\n",
    "    \n",
    "    current_record_list = list(set(list(df1_copy.compound_loc)))\n",
    "    \n",
    "    updated_records_in_new_df = df2_copy[~df2_copy.compound_loc.isin(current_record_list)]\n",
    "    \n",
    "    if len(updated_records_in_new_df) > 0:\n",
    "        updated_records_in_new_df.drop(columns=['compound_loc'], inplace=True)\n",
    "    \n",
    "        return updated_records_in_new_df\n",
    "    else:\n",
    "        empty = []\n",
    "        return empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a99e8f-dd19-4ee1-9739-92252142553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the current records from smartsheet so only new records are added to smartsheet\n",
    "current_smartsheet_records = extract_and_load_to_dataframe_with_row_id(api_key, monthly_billing)\n",
    "current_smartsheet_records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95749e33-0f3d-4fa4-9f0d-c2050b28c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a compound key using the Research_Info__Account_Num and month_year_of_hours_worked to create a unique ID for each record \n",
    "# for the smartsheet data and the Jira data that has been processed so records that have not been added to smartsheet can be identifed\n",
    "# and so that records that are only in smartsheet can be check to see if there have been changes to the hours_worked_in_month from what \n",
    "# has been logged in smartsheet to the newly processsed Jira data\n",
    "\n",
    "# add the file path for the work log record to each row to a seperate df that will be used for processing the required attachements\n",
    "file_path_df = result_df.copy()\n",
    "file_path_df['file_path'] = file_path_df.apply(lambda x: 'delete_me/'+x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked)+'.xlsx', axis=1)\n",
    "file_path_df['compound'] = file_path_df.apply(lambda x: x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked), axis=1)\n",
    "\n",
    "current_smartsheet_records = extract_and_load_to_dataframe_with_row_id(api_key, monthly_billing)\n",
    "\n",
    "# to handle the intial load when there are no current records in smartsheets\n",
    "if len(current_smartsheet_records) == 0:\n",
    "    # add records to smartsheet\n",
    "    add_records_to_smartsheet(result_df, api_key, monthly_billing)\n",
    "    current_smartsheet_records = extract_and_load_to_dataframe_with_row_id(api_key, monthly_billing)\n",
    "    \n",
    "    # create a compound key for linking the smartsheet row id and to identify records that are not currently in smartsheets\n",
    "    result_df['compound'] = result_df.apply(lambda x: x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked), axis=1)\n",
    "    current_smartsheet_records['compound'] = current_smartsheet_records.apply(lambda x: x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked), axis=1)\n",
    "    \n",
    "    # slice out only the required columns from the smartsheet df\n",
    "    smartsheet_df_slice = current_smartsheet_records[['row_id', 'compound']]\n",
    "\n",
    "    # merge the smartsheet slice with the origianl dataframe so the smartsheet row ID is match with each record\n",
    "    merge_filtered = pd.merge(result_df,smartsheet_df_slice, on='compound', how='left')\n",
    "    merge_filtered['file_path'] = merge_filtered.apply(lambda x: 'delete_me/'+x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked)+'.xlsx', axis=1)\n",
    "\n",
    "    # funtion to ensure only int value is available for the row_id\n",
    "    def row_id_int(data):\n",
    "        try:\n",
    "            output = int(data)\n",
    "        except:\n",
    "            output = data\n",
    "\n",
    "        return output\n",
    "\n",
    "    merge_filtered['row_id'] = merge_filtered.apply(lambda x: row_id_int(x.row_id), axis=1)\n",
    "    merge_filtered_not_nan = merge_filtered[~merge_filtered.row_id.isna()]\n",
    "    \n",
    "    counter = 0\n",
    "    df_len = len(merge_filtered_not_nan)\n",
    "    error_list = []\n",
    "\n",
    "    while counter < df_len:\n",
    "        try:\n",
    "            add_excel_file_to_row(api_key, monthly_billing, int(merge_filtered_not_nan.iloc[counter].row_id), merge_filtered_not_nan.iloc[counter].file_path)\n",
    "            counter = counter+1\n",
    "        except:\n",
    "            error_list.append(counter)\n",
    "            counter = counter+1\n",
    "    \n",
    "    \n",
    "else:\n",
    "    # create a compound key for linking the smartsheet row id and to identify records that are not currently in smartsheets\n",
    "    result_df['compound'] = result_df.apply(lambda x: x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked), axis=1)\n",
    "    current_smartsheet_records['compound'] = current_smartsheet_records.apply(lambda x: x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked), axis=1)\n",
    "\n",
    "    # get a list of the compound key from smartsheet and use as a filter to identify new Jira records and Jira records that are currently in smartsheet \n",
    "    records_in_smartsheet_list = list(set(list(current_smartsheet_records.compound)))\n",
    "    new_records_to_smartsheets = result_df[~result_df.compound.isin(records_in_smartsheet_list)]\n",
    "    jira_records_match_smartsheets = result_df[result_df.compound.isin(records_in_smartsheet_list)]\n",
    "    \n",
    "    # QC to see if the smartsheet records have time that is different than what the new Jira data batch contains\n",
    "    qc_monthly_billing = compare_dataframes(current_smartsheet_records, jira_records_match_smartsheets, 'compound', 'hours_worked_in_month')\n",
    "    if len(qc_monthly_billing) > 0:\n",
    "        # add records to Variance Report\n",
    "        qc_monthly_billing.drop(columns=['compound'], inplace=True)\n",
    "        add_records_to_smartsheet(qc_monthly_billing, api_key, variance_report)\n",
    "        # add the file attachments\n",
    "        qc_monthly_billing['compound'] = qc_monthly_billing.apply(lambda x: x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked), axis=1)\n",
    "\n",
    "        current_smartsheet_records = extract_and_load_to_dataframe_with_row_id(api_key, variance_report)\n",
    "        current_smartsheet_records['compound'] = current_smartsheet_records.apply(lambda x: x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked), axis=1)\n",
    "        \n",
    "        # slice out only the required columns from the smartsheet df\n",
    "        smartsheet_df_slice = current_smartsheet_records[['row_id', 'compound']]\n",
    "\n",
    "        # merge the smartsheet slice with the origianl dataframe so the smartsheet row ID is match with each record\n",
    "        merge_filtered = pd.merge(qc_monthly_billing, smartsheet_df_slice, on='compound', how='left')\n",
    "        merge_filtered['file_path'] = merge_filtered.apply(lambda x: 'delete_me/'+x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked)+'.xlsx', axis=1)\n",
    "        \n",
    "        # funtion to ensure only int value is available for the row_id\n",
    "        def row_id_int(data):\n",
    "            try:\n",
    "                output = int(data)\n",
    "            except:\n",
    "                output = data\n",
    "\n",
    "            return output\n",
    "\n",
    "        merge_filtered['row_id'] = merge_filtered.apply(lambda x: row_id_int(x.row_id), axis=1)\n",
    "        merge_filtered_not_nan = merge_filtered[~merge_filtered.row_id.isna()]\n",
    "        merge_filtered_not_nan.head()\n",
    "\n",
    "        counter = 0\n",
    "        df_len = len(merge_filtered_not_nan)\n",
    "        error_list = []\n",
    "\n",
    "        while counter < df_len:\n",
    "            try:\n",
    "                add_excel_file_to_row(api_key, variance_report, int(merge_filtered_not_nan.iloc[counter].row_id), merge_filtered_not_nan.iloc[counter].file_path)\n",
    "                counter = counter+1\n",
    "            except:\n",
    "                error_list.append(counter)\n",
    "                counter = counter+1\n",
    "        \n",
    "        \n",
    "        print('*********************************')\n",
    "        print('*********************************')\n",
    "        print('*********************************')\n",
    "        print('')\n",
    "        print('Records added to Variance Report')\n",
    "        print('')\n",
    "        print('*********************************')\n",
    "        print('*********************************')\n",
    "        print('*********************************')\n",
    "        \n",
    "    # no variance report needed for the monthly billing    \n",
    "    else:\n",
    "        print('no variance report needed for the monthly billing')\n",
    "        \n",
    "    \n",
    "    # add records to Monthly Billing Report if available \n",
    "    if len(new_records_to_smartsheets) > 0:\n",
    "        new_records_to_smartsheets.drop(columns=['compound'], inplace=True)\n",
    "        add_records_to_smartsheet(new_records_to_smartsheets, api_key, monthly_billing)\n",
    "\n",
    "        new_records_to_smartsheets['compound'] = new_records_to_smartsheets.apply(lambda x: x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked), axis=1)\n",
    "\n",
    "        current_smartsheet_records = extract_and_load_to_dataframe_with_row_id(api_key, monthly_billing)\n",
    "        current_smartsheet_records['compound'] = current_smartsheet_records.apply(lambda x: x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked), axis=1)\n",
    "\n",
    "        # slice out only the required columns from the smartsheet df\n",
    "        smartsheet_df_slice = current_smartsheet_records[['row_id', 'compound']]\n",
    "\n",
    "        # merge the smartsheet slice with the origianl dataframe so the smartsheet row ID is match with each record\n",
    "        merge_filtered = pd.merge(new_records_to_smartsheets, smartsheet_df_slice, on='compound', how='left')\n",
    "        merge_filtered['file_path'] = merge_filtered.apply(lambda x: 'delete_me/'+x.Research_Info__Account_Num+\"_\"+str(x.month_year_of_hours_worked)+'.xlsx', axis=1)\n",
    "\n",
    "        # funtion to ensure only int value is available for the row_id\n",
    "        def row_id_int(data):\n",
    "            try:\n",
    "                output = int(data)\n",
    "            except:\n",
    "                output = data\n",
    "\n",
    "            return output\n",
    "\n",
    "        merge_filtered['row_id'] = merge_filtered.apply(lambda x: row_id_int(x.row_id), axis=1)\n",
    "        merge_filtered_not_nan = merge_filtered[~merge_filtered.row_id.isna()]\n",
    "        merge_filtered_not_nan.head()\n",
    "\n",
    "        counter = 0\n",
    "        df_len = len(merge_filtered_not_nan)\n",
    "        error_list = []\n",
    "\n",
    "        while counter < df_len:\n",
    "            try:\n",
    "                add_excel_file_to_row(api_key, monthly_billing, int(merge_filtered_not_nan.iloc[counter].row_id), merge_filtered_not_nan.iloc[counter].file_path)\n",
    "                counter = counter+1\n",
    "            except:\n",
    "                error_list.append(counter)\n",
    "                counter = counter+1\n",
    "    else:\n",
    "        print('no new records to add to the monthly billing report')\n",
    "\n",
    "    \n",
    "    \n",
    "         \n",
    "### this section of code loads the records used to aggrogate the monthly billing into a smartsheet table for reference        \n",
    "        \n",
    "# check the source_billable_records smartsheet data to see if there are new Jira records that need added \n",
    "current_source_billable_smartsheet_records = extract_and_load_to_dataframe_with_row_id(api_key, source_billable_records_sheet)\n",
    "\n",
    "# if statement to handel inital load\n",
    "if len(current_source_billable_smartsheet_records) == 0:\n",
    "    final_record_set['time_record_updated'] = 0\n",
    "    add_records_to_smartsheet(final_record_set, api_key, source_billable_records_sheet)\n",
    "    \n",
    "else:\n",
    "    current_source_billable_smartsheet_records['WorkLog_ID'] = current_source_billable_smartsheet_records['WorkLog_ID'].astype(int)\n",
    "    current_source_billable_smartsheet_records['Time_Worked_Hours'] = current_source_billable_smartsheet_records['Time_Worked_Hours'].astype(float)\n",
    "    billable_worklog_ids_list = list(set(list(current_source_billable_smartsheet_records.WorkLog_ID)))\n",
    "    \n",
    "    new_billable_records = final_record_set.copy()\n",
    "    new_billable_records_to_process = new_billable_records[~new_billable_records.WorkLog_ID.isin(billable_worklog_ids_list)] # these will be loaded into smartsheets\n",
    "    prior_processes_records = new_billable_records[new_billable_records.WorkLog_ID.isin(billable_worklog_ids_list)] # these will be checked if updates have been made to the time records\n",
    "    \n",
    "    if len(new_billable_records_to_process) > 0: \n",
    "        print('Load new billable time records to smartsheet')\n",
    "        new_billable_records_to_process['time_record_updated'] = 0\n",
    "        add_records_to_smartsheet(new_billable_records_to_process, api_key, source_billable_records_sheet)\n",
    "    else:\n",
    "        print('no billable records to add to the report')\n",
    "    \n",
    "    # QC to see if time records have been updated and if they have then add them to the table with a flag time_record_updated set to 1\n",
    "    updated_rows = compare_dataframes(current_source_billable_smartsheet_records, prior_processes_records, 'WorkLog_ID', 'Time_Worked_Hours')\n",
    "    if len(updated_rows) > 0:\n",
    "        updated_rows['time_record_updated'] = 1\n",
    "        # add to smartsheets\n",
    "        print('Time records have been changed and the record has been posted')\n",
    "        add_records_to_smartsheet(updated_rows, api_key, source_billable_records_sheet)\n",
    "    else:\n",
    "        print('no updates to time records found')\n",
    "        \n",
    "        \n",
    "print('*********************************')\n",
    "print('*********************************')\n",
    "print('*********************************')\n",
    "print('')\n",
    "print('Reminder to delete the delete me folder')\n",
    "print('')\n",
    "print('*********************************')\n",
    "print('*********************************')\n",
    "print('*********************************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
